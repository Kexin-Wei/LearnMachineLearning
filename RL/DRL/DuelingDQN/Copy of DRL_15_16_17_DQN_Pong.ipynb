{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of DRL_15_16_17_DQN_Pong.ipynb","provenance":[{"file_id":"https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb","timestamp":1608972483800}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"40Yb47zJQglm"},"source":["DEEP REINFORCEMENT LEARNING EXPLAINED - 15 - 16 - 17\n","# **Deep Q-Network (DQN)**"]},{"cell_type":"markdown","metadata":{"id":"Q40Fa7qM4_lE"},"source":["OpenAI Pong"]},{"cell_type":"code","metadata":{"id":"FA1Y5VCv20XZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609147579308,"user_tz":-480,"elapsed":1638,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}},"outputId":"5cf47043-a0a8-4d45-d20b-e542c7bcc265"},"source":["import gym\n","import gym.spaces\n","\n","DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n","test_env = gym.make(DEFAULT_ENV_NAME)\n","print(test_env.action_space.n)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8QDaXip14JBv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609147580676,"user_tz":-480,"elapsed":641,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}},"outputId":"8e36ec5a-ffcf-4246-c3da-671cdf3f9977"},"source":["print(test_env.unwrapped.get_action_meanings())"],"execution_count":2,"outputs":[{"output_type":"stream","text":["['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1uzLQLz04z2i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609147583342,"user_tz":-480,"elapsed":922,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}},"outputId":"5a6d61bf-e9f1-4b07-fefe-d4abffc36226"},"source":["print(test_env.observation_space.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(210, 160, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZzcdmzIL5EMI"},"source":["\n","Type of hardware accelerator provided by Colab"]},{"cell_type":"code","metadata":{"id":"VjUM99rEKFNt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609147587375,"user_tz":-480,"elapsed":972,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}},"outputId":"cfe33b21-ce46-4543-9d40-defd9eb3286e"},"source":["!nvidia-smi "],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mon Dec 28 09:26:26 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   57C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZhmsqgrHikEl","executionInfo":{"status":"ok","timestamp":1609147589406,"user_tz":-480,"elapsed":1037,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}}},"source":["import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pRcuJGVSQi6g"},"source":["## OpenAI Gym Wrappers"]},{"cell_type":"code","metadata":{"id":"nPi1lHINMuSu","executionInfo":{"status":"ok","timestamp":1609147631385,"user_tz":-480,"elapsed":1701,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}}},"source":["# Taken from \n","# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n","\n","import cv2\n","import numpy as np\n","import collections\n","\n","class FireResetEnv(gym.Wrapper):\n","    def __init__(self, env=None):\n","        super(FireResetEnv, self).__init__(env)\n","        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n","        assert len(env.unwrapped.get_action_meanings()) >= 3\n","\n","    def step(self, action):\n","        return self.env.step(action)\n","\n","    def reset(self):\n","        self.env.reset()\n","        obs, _, done, _ = self.env.step(1)\n","        if done:\n","            self.env.reset()\n","        obs, _, done, _ = self.env.step(2)\n","        if done:\n","            self.env.reset()\n","        return obs\n","\n","class MaxAndSkipEnv(gym.Wrapper):\n","    def __init__(self, env=None, skip=4):\n","        super(MaxAndSkipEnv, self).__init__(env)\n","        # most recent raw observations (for max pooling across time steps)\n","        self._obs_buffer = collections.deque(maxlen=2)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","        done = None\n","        for _ in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            self._obs_buffer.append(obs)\n","            total_reward += reward\n","            if done:\n","                break\n","        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n","        return max_frame, total_reward, done, info\n","\n","    def reset(self):\n","        self._obs_buffer.clear()\n","        obs = self.env.reset()\n","        self._obs_buffer.append(obs)\n","        return obs\n","\n","class ProcessFrame84(gym.ObservationWrapper):\n","    def __init__(self, env=None):\n","        super(ProcessFrame84, self).__init__(env)\n","        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n","\n","    def observation(self, obs):\n","        return ProcessFrame84.process(obs)\n","\n","    @staticmethod\n","    def process(frame):\n","        if frame.size == 210 * 160 * 3:\n","            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n","        elif frame.size == 250 * 160 * 3:\n","            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n","        else:\n","            assert False, \"Unknown resolution.\"\n","        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n","        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n","        x_t = resized_screen[18:102, :]\n","        x_t = np.reshape(x_t, [84, 84, 1])\n","        return x_t.astype(np.uint8)\n","\n","\n","class BufferWrapper(gym.ObservationWrapper):\n","    def __init__(self, env, n_steps, dtype=np.float32):\n","        super(BufferWrapper, self).__init__(env)\n","        self.dtype = dtype\n","        old_space = env.observation_space\n","        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n","                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n","\n","    def reset(self):\n","        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n","        return self.observation(self.env.reset())\n","\n","    def observation(self, observation):\n","        self.buffer[:-1] = self.buffer[1:]\n","        self.buffer[-1] = observation\n","        return self.buffer\n","\n","\n","class ImageToPyTorch(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super(ImageToPyTorch, self).__init__(env)\n","        old_shape = self.observation_space.shape\n","        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n","                                old_shape[0], old_shape[1]), dtype=np.float32)\n","\n","    def observation(self, observation):\n","        return np.moveaxis(observation, 2, 0)\n","\n","\n","class ScaledFloatFrame(gym.ObservationWrapper):\n","    def observation(self, obs):\n","        return np.array(obs).astype(np.float32) / 255.0\n","\n","def make_env(env_name):\n","    env = gym.make(env_name)\n","    env = MaxAndSkipEnv(env)\n","    env = FireResetEnv(env)\n","    env = ProcessFrame84(env)\n","    env = ImageToPyTorch(env)\n","    env = BufferWrapper(env, 4)\n","    return ScaledFloatFrame(env)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wznv9I1KR_I3"},"source":["## The DQN model\n"]},{"cell_type":"code","metadata":{"id":"h6B8v-Qh5Ykk","executionInfo":{"status":"ok","timestamp":1609147637354,"user_tz":-480,"elapsed":5061,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}}},"source":["import torch\n","import torch.nn as nn        # Pytorch neural network package\n","import torch.optim as optim  # Pytorch optimization package\n","\n","device = torch.device(\"cuda\")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4S1I9xWMkf3","executionInfo":{"status":"ok","timestamp":1609147641867,"user_tz":-480,"elapsed":1022,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}}},"source":["# Taken from \n","# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n","\n","import numpy as np\n","\n","class DQN(nn.Module):\n","    def __init__(self, input_shape, n_actions):\n","        super(DQN, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU()\n","        )\n","\n","        conv_out_size = self._get_conv_out(input_shape)\n","        self.fc = nn.Sequential(\n","            nn.Linear(conv_out_size, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, n_actions)\n","        )\n","\n","    def _get_conv_out(self, shape):\n","        o = self.conv(torch.zeros(1, *shape))\n","        return int(np.prod(o.size()))\n","\n","    def forward(self, x):\n","        conv_out = self.conv(x).view(x.size()[0], -1)\n","        return self.fc(conv_out)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"taYi5LZnIOqz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609147657371,"user_tz":-480,"elapsed":11656,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}},"outputId":"05f4c2e9-8731-4abd-e20c-0f5a6cb6a365"},"source":["test_env = make_env(DEFAULT_ENV_NAME)\n","test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n","print(test_net)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["DQN(\n","  (conv): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n","    (3): ReLU()\n","    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (5): ReLU()\n","  )\n","  (fc): Sequential(\n","    (0): Linear(in_features=3136, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=6, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lhv3Yf-aW7UW"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"aPJl73Z1YTa4"},"source":["Load Tensorboard extension"]},{"cell_type":"code","metadata":{"id":"BCBQhXLfNeUG","executionInfo":{"status":"ok","timestamp":1609147663819,"user_tz":-480,"elapsed":2445,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}}},"source":["from torch.utils.tensorboard import SummaryWriter\n","%load_ext tensorboard"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kb_f_onMXkpb"},"source":["Import required modules and define the hyperparameters"]},{"cell_type":"code","metadata":{"id":"AGwHC9dyXoPd","executionInfo":{"status":"ok","timestamp":1609147666131,"user_tz":-480,"elapsed":1013,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}}},"source":["import time\n","import numpy as np\n","import collections\n","\n","\n","MEAN_REWARD_BOUND = 19.0           \n","\n","gamma = 0.99                   \n","batch_size = 32                \n","replay_size = 10000            \n","learning_rate = 1e-4           \n","sync_target_frames = 1000      \n","replay_start_size = 10000      \n","\n","eps_start=1.0\n","eps_decay=.999985\n","eps_min=0.02"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FFaMmDKqYmo4"},"source":["Experience replay buffer"]},{"cell_type":"code","metadata":{"id":"Y79CNYsjY4w0","executionInfo":{"status":"ok","timestamp":1609147671562,"user_tz":-480,"elapsed":4132,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}}},"source":["Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n","\n","class ExperienceReplay:\n","    def __init__(self, capacity):\n","        self.buffer = collections.deque(maxlen=capacity)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    def append(self, experience):\n","        self.buffer.append(experience)\n","\n","    def sample(self, batch_size):\n","        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n","        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n","        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n","               np.array(dones, dtype=np.uint8), np.array(next_states)\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQDV04ktY3xs"},"source":["Agent"]},{"cell_type":"code","metadata":{"id":"YdAKFiMWZw90","executionInfo":{"status":"ok","timestamp":1609147675361,"user_tz":-480,"elapsed":2180,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}}},"source":["class Agent:\n","    def __init__(self, env, exp_buffer):\n","        self.env = env\n","        self.exp_buffer = exp_buffer\n","        self._reset()\n","\n","    def _reset(self):\n","        self.state = env.reset()\n","        self.total_reward = 0.0\n","\n","    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n","\n","        done_reward = None\n","        if np.random.random() < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            state_a = np.array([self.state], copy=False)\n","            state_v = torch.tensor(state_a).to(device)\n","            q_vals_v = net(state_v)\n","            print(q_vals_v)\n","            _, act_v = torch.max(q_vals_v, dim=1)\n","            action = int(act_v.item())\n","\n","        new_state, reward, is_done, _ = self.env.step(action)\n","        self.total_reward += reward\n","\n","        exp = Experience(self.state, action, reward, is_done, new_state)\n","        self.exp_buffer.append(exp)\n","        self.state = new_state\n","        if is_done:\n","            done_reward = self.total_reward\n","            self._reset()\n","        return done_reward\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"ipurwYpa6iKn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609147682084,"user_tz":-480,"elapsed":4641,"user":{"displayName":"Cation Vac","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXFGQsX6ZQ6rWUBLp8WBz1xySc63FB3XJhWjbr=s64","userId":"07548202097277871177"}},"outputId":"45a44937-2847-4de9-9930-10649b7ae398"},"source":["import datetime\n","print(\">>>Training starts at \",datetime.datetime.now())"],"execution_count":15,"outputs":[{"output_type":"stream","text":[">>>Training starts at  2020-12-28 09:27:57.683366\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bgpmAtchZwM_"},"source":["Main training loop"]},{"cell_type":"code","metadata":{"id":"qEoc2PWmM2mu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"68596563-3988-41b3-a19f-1cffefeef4ef"},"source":["env = make_env(DEFAULT_ENV_NAME)\n","\n","net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n","target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n","writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n"," \n","buffer = ExperienceReplay(replay_size)\n","agent = Agent(env, buffer)\n","\n","epsilon = eps_start\n","\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","total_rewards = []\n","frame_idx = 0  \n","\n","best_mean_reward = None\n","\n","while True:\n","        frame_idx += 1\n","        epsilon = max(epsilon*eps_decay, eps_min)\n","\n","        reward = agent.play_step(net, epsilon, device=device)\n","        if reward is not None:\n","            total_rewards.append(reward)\n","\n","            mean_reward = np.mean(total_rewards[-100:])\n","\n","            print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\n","                frame_idx, len(total_rewards), mean_reward, epsilon))\n","            \n","            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n","            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n","            writer.add_scalar(\"reward\", reward, frame_idx)\n","\n","            if best_mean_reward is None or best_mean_reward < mean_reward:\n","                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n","                best_mean_reward = mean_reward\n","                if best_mean_reward is not None:\n","                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n","\n","            if mean_reward > MEAN_REWARD_BOUND:\n","                print(\"Solved in %d frames!\" % frame_idx)\n","                break\n","\n","        if len(buffer) < replay_start_size:\n","            continue\n","\n","        batch = buffer.sample(batch_size)\n","        states, actions, rewards, dones, next_states = batch\n","\n","        states_v = torch.tensor(states).to(device)\n","        next_states_v = torch.tensor(next_states).to(device)\n","        actions_v = torch.tensor(actions).to(device)\n","        rewards_v = torch.tensor(rewards).to(device)\n","        done_mask = torch.ByteTensor(dones).to(device)\n","\n","        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n","\n","        next_state_values = target_net(next_states_v).max(1)[0]\n","\n","        next_state_values[done_mask] = 0.0\n","\n","        next_state_values = next_state_values.detach()\n","\n","        expected_state_action_values = next_state_values * gamma + rewards_v\n","\n","        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n","\n","        optimizer.zero_grad()\n","        loss_t.backward()\n","        optimizer.step()\n","\n","        if frame_idx % sync_target_frames == 0:\n","            target_net.load_state_dict(net.state_dict())\n","       \n","writer.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["947:  1 games, mean reward -20.000, (epsilon 0.99)\n","Best mean reward updated -20.000\n","1797:  2 games, mean reward -20.500, (epsilon 0.97)\n","2805:  3 games, mean reward -20.333, (epsilon 0.96)\n","3795:  4 games, mean reward -20.250, (epsilon 0.94)\n","4784:  5 games, mean reward -20.200, (epsilon 0.93)\n","5634:  6 games, mean reward -20.333, (epsilon 0.92)\n","6396:  7 games, mean reward -20.429, (epsilon 0.91)\n","7296:  8 games, mean reward -20.500, (epsilon 0.90)\n","8197:  9 games, mean reward -20.444, (epsilon 0.88)\n","9116:  10 games, mean reward -20.400, (epsilon 0.87)\n","9938:  11 games, mean reward -20.455, (epsilon 0.86)\n","10728:  12 games, mean reward -20.500, (epsilon 0.85)\n","11624:  13 games, mean reward -20.462, (epsilon 0.84)\n","12492:  14 games, mean reward -20.429, (epsilon 0.83)\n","13394:  15 games, mean reward -20.400, (epsilon 0.82)\n","14373:  16 games, mean reward -20.375, (epsilon 0.81)\n","15390:  17 games, mean reward -20.353, (epsilon 0.79)\n","16180:  18 games, mean reward -20.389, (epsilon 0.78)\n","17002:  19 games, mean reward -20.421, (epsilon 0.77)\n","18034:  20 games, mean reward -20.300, (epsilon 0.76)\n","18902:  21 games, mean reward -20.286, (epsilon 0.75)\n","19862:  22 games, mean reward -20.273, (epsilon 0.74)\n","20750:  23 games, mean reward -20.261, (epsilon 0.73)\n","21651:  24 games, mean reward -20.292, (epsilon 0.72)\n","22473:  25 games, mean reward -20.320, (epsilon 0.71)\n","23523:  26 games, mean reward -20.346, (epsilon 0.70)\n","24454:  27 games, mean reward -20.333, (epsilon 0.69)\n","25216:  28 games, mean reward -20.357, (epsilon 0.69)\n","26233:  29 games, mean reward -20.345, (epsilon 0.67)\n","27134:  30 games, mean reward -20.333, (epsilon 0.67)\n","28158:  31 games, mean reward -20.290, (epsilon 0.66)\n","29147:  32 games, mean reward -20.281, (epsilon 0.65)\n","30057:  33 games, mean reward -20.303, (epsilon 0.64)\n","30982:  34 games, mean reward -20.294, (epsilon 0.63)\n","31804:  35 games, mean reward -20.314, (epsilon 0.62)\n","32654:  36 games, mean reward -20.333, (epsilon 0.61)\n","33969:  37 games, mean reward -20.297, (epsilon 0.60)\n","34731:  38 games, mean reward -20.316, (epsilon 0.59)\n","35668:  39 games, mean reward -20.333, (epsilon 0.59)\n","36593:  40 games, mean reward -20.325, (epsilon 0.58)\n","37383:  41 games, mean reward -20.341, (epsilon 0.57)\n","38252:  42 games, mean reward -20.333, (epsilon 0.56)\n","39167:  43 games, mean reward -20.349, (epsilon 0.56)\n","39986:  44 games, mean reward -20.364, (epsilon 0.55)\n","40868:  45 games, mean reward -20.378, (epsilon 0.54)\n","41767:  46 games, mean reward -20.391, (epsilon 0.53)\n","42684:  47 games, mean reward -20.383, (epsilon 0.53)\n","43493:  48 games, mean reward -20.396, (epsilon 0.52)\n","44463:  49 games, mean reward -20.388, (epsilon 0.51)\n","45543:  50 games, mean reward -20.380, (epsilon 0.51)\n","46581:  51 games, mean reward -20.373, (epsilon 0.50)\n","47739:  52 games, mean reward -20.346, (epsilon 0.49)\n","48743:  53 games, mean reward -20.358, (epsilon 0.48)\n","49878:  54 games, mean reward -20.352, (epsilon 0.47)\n","50920:  55 games, mean reward -20.364, (epsilon 0.47)\n","52070:  56 games, mean reward -20.321, (epsilon 0.46)\n","53250:  57 games, mean reward -20.316, (epsilon 0.45)\n","54544:  58 games, mean reward -20.293, (epsilon 0.44)\n","55819:  59 games, mean reward -20.288, (epsilon 0.43)\n","57417:  60 games, mean reward -20.233, (epsilon 0.42)\n","58473:  61 games, mean reward -20.230, (epsilon 0.42)\n","59830:  62 games, mean reward -20.210, (epsilon 0.41)\n","61204:  63 games, mean reward -20.175, (epsilon 0.40)\n","63052:  64 games, mean reward -20.109, (epsilon 0.39)\n","64346:  65 games, mean reward -20.092, (epsilon 0.38)\n","66048:  66 games, mean reward -20.045, (epsilon 0.37)\n","67692:  67 games, mean reward -20.015, (epsilon 0.36)\n","69369:  68 games, mean reward -19.926, (epsilon 0.35)\n","Best mean reward updated -19.926\n","70856:  69 games, mean reward -19.899, (epsilon 0.35)\n","Best mean reward updated -19.899\n","72303:  70 games, mean reward -19.857, (epsilon 0.34)\n","Best mean reward updated -19.857\n","73969:  71 games, mean reward -19.817, (epsilon 0.33)\n","Best mean reward updated -19.817\n","75504:  72 games, mean reward -19.764, (epsilon 0.32)\n","Best mean reward updated -19.764\n","77157:  73 games, mean reward -19.740, (epsilon 0.31)\n","Best mean reward updated -19.740\n","79167:  74 games, mean reward -19.676, (epsilon 0.30)\n","Best mean reward updated -19.676\n","81255:  75 games, mean reward -19.587, (epsilon 0.30)\n","Best mean reward updated -19.587\n","83435:  76 games, mean reward -19.474, (epsilon 0.29)\n","Best mean reward updated -19.474\n","85457:  77 games, mean reward -19.455, (epsilon 0.28)\n","Best mean reward updated -19.455\n","87596:  78 games, mean reward -19.372, (epsilon 0.27)\n","Best mean reward updated -19.372\n","89675:  79 games, mean reward -19.304, (epsilon 0.26)\n","Best mean reward updated -19.304\n","91448:  80 games, mean reward -19.262, (epsilon 0.25)\n","Best mean reward updated -19.262\n","93820:  81 games, mean reward -19.185, (epsilon 0.24)\n","Best mean reward updated -19.185\n","95527:  82 games, mean reward -19.183, (epsilon 0.24)\n","Best mean reward updated -19.183\n","97046:  83 games, mean reward -19.181, (epsilon 0.23)\n","Best mean reward updated -19.181\n","98722:  84 games, mean reward -19.131, (epsilon 0.23)\n","Best mean reward updated -19.131\n","100196:  85 games, mean reward -19.141, (epsilon 0.22)\n","102366:  86 games, mean reward -19.070, (epsilon 0.22)\n","Best mean reward updated -19.070\n","104212:  87 games, mean reward -19.057, (epsilon 0.21)\n","Best mean reward updated -19.057\n","105922:  88 games, mean reward -19.057, (epsilon 0.20)\n","Best mean reward updated -19.057\n","107907:  89 games, mean reward -18.966, (epsilon 0.20)\n","Best mean reward updated -18.966\n","109792:  90 games, mean reward -18.933, (epsilon 0.19)\n","Best mean reward updated -18.933\n","111929:  91 games, mean reward -18.879, (epsilon 0.19)\n","Best mean reward updated -18.879\n","113983:  92 games, mean reward -18.848, (epsilon 0.18)\n","Best mean reward updated -18.848\n","116105:  93 games, mean reward -18.796, (epsilon 0.18)\n","Best mean reward updated -18.796\n","117953:  94 games, mean reward -18.777, (epsilon 0.17)\n","Best mean reward updated -18.777\n","119838:  95 games, mean reward -18.737, (epsilon 0.17)\n","Best mean reward updated -18.737\n","121795:  96 games, mean reward -18.719, (epsilon 0.16)\n","Best mean reward updated -18.719\n","123963:  97 games, mean reward -18.680, (epsilon 0.16)\n","Best mean reward updated -18.680\n","125899:  98 games, mean reward -18.663, (epsilon 0.15)\n","Best mean reward updated -18.663\n","128481:  99 games, mean reward -18.586, (epsilon 0.15)\n","Best mean reward updated -18.586\n","130557:  100 games, mean reward -18.540, (epsilon 0.14)\n","Best mean reward updated -18.540\n","132549:  101 games, mean reward -18.520, (epsilon 0.14)\n","Best mean reward updated -18.520\n","134865:  102 games, mean reward -18.460, (epsilon 0.13)\n","Best mean reward updated -18.460\n","137733:  103 games, mean reward -18.400, (epsilon 0.13)\n","Best mean reward updated -18.400\n","140746:  104 games, mean reward -18.310, (epsilon 0.12)\n","Best mean reward updated -18.310\n","143246:  105 games, mean reward -18.280, (epsilon 0.12)\n","Best mean reward updated -18.280\n","145035:  106 games, mean reward -18.250, (epsilon 0.11)\n","Best mean reward updated -18.250\n","147340:  107 games, mean reward -18.190, (epsilon 0.11)\n","Best mean reward updated -18.190\n","149751:  108 games, mean reward -18.150, (epsilon 0.11)\n","Best mean reward updated -18.150\n","152091:  109 games, mean reward -18.090, (epsilon 0.10)\n","Best mean reward updated -18.090\n","154141:  110 games, mean reward -18.070, (epsilon 0.10)\n","Best mean reward updated -18.070\n","157047:  111 games, mean reward -18.000, (epsilon 0.09)\n","Best mean reward updated -18.000\n","160005:  112 games, mean reward -17.910, (epsilon 0.09)\n","Best mean reward updated -17.910\n","162572:  113 games, mean reward -17.830, (epsilon 0.09)\n","Best mean reward updated -17.830\n","165191:  114 games, mean reward -17.790, (epsilon 0.08)\n","Best mean reward updated -17.790\n","167875:  115 games, mean reward -17.740, (epsilon 0.08)\n","Best mean reward updated -17.740\n","171860:  116 games, mean reward -17.600, (epsilon 0.08)\n","Best mean reward updated -17.600\n","174514:  117 games, mean reward -17.530, (epsilon 0.07)\n","Best mean reward updated -17.530\n","178031:  118 games, mean reward -17.420, (epsilon 0.07)\n","Best mean reward updated -17.420\n","181479:  119 games, mean reward -17.230, (epsilon 0.07)\n","Best mean reward updated -17.230\n","183945:  120 games, mean reward -17.200, (epsilon 0.06)\n","Best mean reward updated -17.200\n","187023:  121 games, mean reward -17.120, (epsilon 0.06)\n","Best mean reward updated -17.120\n","190142:  122 games, mean reward -16.970, (epsilon 0.06)\n","Best mean reward updated -16.970\n","192265:  123 games, mean reward -16.920, (epsilon 0.06)\n","Best mean reward updated -16.920\n","195047:  124 games, mean reward -16.760, (epsilon 0.05)\n","Best mean reward updated -16.760\n","198577:  125 games, mean reward -16.590, (epsilon 0.05)\n","Best mean reward updated -16.590\n","201637:  126 games, mean reward -16.440, (epsilon 0.05)\n","Best mean reward updated -16.440\n","204845:  127 games, mean reward -16.290, (epsilon 0.05)\n","Best mean reward updated -16.290\n","208387:  128 games, mean reward -16.130, (epsilon 0.04)\n","Best mean reward updated -16.130\n","211930:  129 games, mean reward -15.950, (epsilon 0.04)\n","Best mean reward updated -15.950\n","214786:  130 games, mean reward -15.680, (epsilon 0.04)\n","Best mean reward updated -15.680\n","217808:  131 games, mean reward -15.440, (epsilon 0.04)\n","Best mean reward updated -15.440\n","221130:  132 games, mean reward -15.280, (epsilon 0.04)\n","Best mean reward updated -15.280\n","225222:  133 games, mean reward -15.080, (epsilon 0.03)\n","Best mean reward updated -15.080\n","227701:  134 games, mean reward -14.740, (epsilon 0.03)\n","Best mean reward updated -14.740\n","231294:  135 games, mean reward -14.570, (epsilon 0.03)\n","Best mean reward updated -14.570\n","233576:  136 games, mean reward -14.190, (epsilon 0.03)\n","Best mean reward updated -14.190\n","236639:  137 games, mean reward -13.900, (epsilon 0.03)\n","Best mean reward updated -13.900\n","239719:  138 games, mean reward -13.600, (epsilon 0.03)\n","Best mean reward updated -13.600\n","242084:  139 games, mean reward -13.220, (epsilon 0.03)\n","Best mean reward updated -13.220\n","244352:  140 games, mean reward -12.860, (epsilon 0.03)\n","Best mean reward updated -12.860\n","247405:  141 games, mean reward -12.600, (epsilon 0.02)\n","Best mean reward updated -12.600\n","250260:  142 games, mean reward -12.320, (epsilon 0.02)\n","Best mean reward updated -12.320\n","252941:  143 games, mean reward -12.000, (epsilon 0.02)\n","Best mean reward updated -12.000\n","255348:  144 games, mean reward -11.650, (epsilon 0.02)\n","Best mean reward updated -11.650\n","258658:  145 games, mean reward -11.360, (epsilon 0.02)\n","Best mean reward updated -11.360\n","261187:  146 games, mean reward -11.010, (epsilon 0.02)\n","Best mean reward updated -11.010\n","264057:  147 games, mean reward -10.700, (epsilon 0.02)\n","Best mean reward updated -10.700\n","266928:  148 games, mean reward -10.370, (epsilon 0.02)\n","Best mean reward updated -10.370\n","269506:  149 games, mean reward -10.010, (epsilon 0.02)\n","Best mean reward updated -10.010\n","271960:  150 games, mean reward -9.660, (epsilon 0.02)\n","Best mean reward updated -9.660\n","274114:  151 games, mean reward -9.260, (epsilon 0.02)\n","Best mean reward updated -9.260\n","276692:  152 games, mean reward -8.910, (epsilon 0.02)\n","Best mean reward updated -8.910\n","279149:  153 games, mean reward -8.540, (epsilon 0.02)\n","Best mean reward updated -8.540\n","281699:  154 games, mean reward -8.190, (epsilon 0.02)\n","Best mean reward updated -8.190\n","283991:  155 games, mean reward -7.830, (epsilon 0.02)\n","Best mean reward updated -7.830\n","286216:  156 games, mean reward -7.460, (epsilon 0.02)\n","Best mean reward updated -7.460\n","288315:  157 games, mean reward -7.060, (epsilon 0.02)\n","Best mean reward updated -7.060\n","290420:  158 games, mean reward -6.680, (epsilon 0.02)\n","Best mean reward updated -6.680\n","292836:  159 games, mean reward -6.310, (epsilon 0.02)\n","Best mean reward updated -6.310\n","295472:  160 games, mean reward -5.980, (epsilon 0.02)\n","Best mean reward updated -5.980\n","297186:  161 games, mean reward -5.940, (epsilon 0.02)\n","Best mean reward updated -5.940\n","301052:  162 games, mean reward -5.740, (epsilon 0.02)\n","Best mean reward updated -5.740\n","303407:  163 games, mean reward -5.370, (epsilon 0.02)\n","Best mean reward updated -5.370\n","305741:  164 games, mean reward -5.070, (epsilon 0.02)\n","Best mean reward updated -5.070\n","308152:  165 games, mean reward -4.710, (epsilon 0.02)\n","Best mean reward updated -4.710\n","311111:  166 games, mean reward -4.420, (epsilon 0.02)\n","Best mean reward updated -4.420\n","313527:  167 games, mean reward -4.100, (epsilon 0.02)\n","Best mean reward updated -4.100\n","315728:  168 games, mean reward -3.800, (epsilon 0.02)\n","Best mean reward updated -3.800\n","318809:  169 games, mean reward -3.560, (epsilon 0.02)\n","Best mean reward updated -3.560\n","321721:  170 games, mean reward -3.270, (epsilon 0.02)\n","Best mean reward updated -3.270\n","324415:  171 games, mean reward -2.960, (epsilon 0.02)\n","Best mean reward updated -2.960\n","327179:  172 games, mean reward -2.700, (epsilon 0.02)\n","Best mean reward updated -2.700\n","329359:  173 games, mean reward -2.340, (epsilon 0.02)\n","Best mean reward updated -2.340\n","332483:  174 games, mean reward -2.090, (epsilon 0.02)\n","Best mean reward updated -2.090\n","335119:  175 games, mean reward -1.810, (epsilon 0.02)\n","Best mean reward updated -1.810\n","337835:  176 games, mean reward -1.570, (epsilon 0.02)\n","Best mean reward updated -1.570\n","340261:  177 games, mean reward -1.250, (epsilon 0.02)\n","Best mean reward updated -1.250\n","342824:  178 games, mean reward -1.000, (epsilon 0.02)\n","Best mean reward updated -1.000\n","344935:  179 games, mean reward -0.660, (epsilon 0.02)\n","Best mean reward updated -0.660\n","347481:  180 games, mean reward -0.350, (epsilon 0.02)\n","Best mean reward updated -0.350\n","350013:  181 games, mean reward -0.070, (epsilon 0.02)\n","Best mean reward updated -0.070\n","352657:  182 games, mean reward 0.260, (epsilon 0.02)\n","Best mean reward updated 0.260\n","355259:  183 games, mean reward 0.580, (epsilon 0.02)\n","Best mean reward updated 0.580\n","357749:  184 games, mean reward 0.880, (epsilon 0.02)\n","Best mean reward updated 0.880\n","360795:  185 games, mean reward 1.190, (epsilon 0.02)\n","Best mean reward updated 1.190\n","363752:  186 games, mean reward 1.410, (epsilon 0.02)\n","Best mean reward updated 1.410\n","366070:  187 games, mean reward 1.740, (epsilon 0.02)\n","Best mean reward updated 1.740\n","369266:  188 games, mean reward 2.050, (epsilon 0.02)\n","Best mean reward updated 2.050\n","371457:  189 games, mean reward 2.330, (epsilon 0.02)\n","Best mean reward updated 2.330\n","373983:  190 games, mean reward 2.630, (epsilon 0.02)\n","Best mean reward updated 2.630\n","376580:  191 games, mean reward 2.900, (epsilon 0.02)\n","Best mean reward updated 2.900\n","378623:  192 games, mean reward 3.260, (epsilon 0.02)\n","Best mean reward updated 3.260\n","380796:  193 games, mean reward 3.590, (epsilon 0.02)\n","Best mean reward updated 3.590\n","383378:  194 games, mean reward 3.910, (epsilon 0.02)\n","Best mean reward updated 3.910\n","385442:  195 games, mean reward 4.250, (epsilon 0.02)\n","Best mean reward updated 4.250\n","387582:  196 games, mean reward 4.590, (epsilon 0.02)\n","Best mean reward updated 4.590\n","390026:  197 games, mean reward 4.900, (epsilon 0.02)\n","Best mean reward updated 4.900\n","392996:  198 games, mean reward 5.140, (epsilon 0.02)\n","Best mean reward updated 5.140\n","395015:  199 games, mean reward 5.440, (epsilon 0.02)\n","Best mean reward updated 5.440\n","397439:  200 games, mean reward 5.720, (epsilon 0.02)\n","Best mean reward updated 5.720\n","400184:  201 games, mean reward 6.020, (epsilon 0.02)\n","Best mean reward updated 6.020\n","402252:  202 games, mean reward 6.360, (epsilon 0.02)\n","Best mean reward updated 6.360\n","404584:  203 games, mean reward 6.680, (epsilon 0.02)\n","Best mean reward updated 6.680\n","407262:  204 games, mean reward 6.910, (epsilon 0.02)\n","Best mean reward updated 6.910\n","409310:  205 games, mean reward 7.280, (epsilon 0.02)\n","Best mean reward updated 7.280\n","411554:  206 games, mean reward 7.640, (epsilon 0.02)\n","Best mean reward updated 7.640\n","413628:  207 games, mean reward 7.970, (epsilon 0.02)\n","Best mean reward updated 7.970\n","415779:  208 games, mean reward 8.310, (epsilon 0.02)\n","Best mean reward updated 8.310\n","418927:  209 games, mean reward 8.550, (epsilon 0.02)\n","Best mean reward updated 8.550\n","422043:  210 games, mean reward 8.700, (epsilon 0.02)\n","Best mean reward updated 8.700\n","424770:  211 games, mean reward 8.920, (epsilon 0.02)\n","Best mean reward updated 8.920\n","428307:  212 games, mean reward 9.070, (epsilon 0.02)\n","Best mean reward updated 9.070\n","430915:  213 games, mean reward 9.280, (epsilon 0.02)\n","Best mean reward updated 9.280\n","434026:  214 games, mean reward 9.510, (epsilon 0.02)\n","Best mean reward updated 9.510\n","436689:  215 games, mean reward 9.770, (epsilon 0.02)\n","Best mean reward updated 9.770\n","439543:  216 games, mean reward 9.940, (epsilon 0.02)\n","Best mean reward updated 9.940\n","442785:  217 games, mean reward 10.140, (epsilon 0.02)\n","Best mean reward updated 10.140\n","445182:  218 games, mean reward 10.340, (epsilon 0.02)\n","Best mean reward updated 10.340\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MZPkszw66cmO"},"source":["print(\">>>Training ends at \",datetime.datetime.now())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qNH2N64k3QRz"},"source":["Performance"]},{"cell_type":"code","metadata":{"id":"WKbcwfK321Hl"},"source":["tensorboard  --logdir=runs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6p0jvxoC3m5W"},"source":["## Using the model"]},{"cell_type":"code","metadata":{"id":"TLEfbkKl6AZV"},"source":["import gym\n","import time\n","import numpy as np\n","\n","import torch\n","\n","import collections\n","\n","DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n","FPS = 25"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4m0Vm4Yp91ZI"},"source":["Tunning the image rendering in colab\n"]},{"cell_type":"code","metadata":{"id":"kgpHXywd5SyZ"},"source":["# Taken from \n","# https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n","\n","!apt-get install -y xvfb x11-utils\n","\n","!pip install pyvirtualdisplay==0.2.* \\\n","             PyOpenGL==3.1.* \\\n","             PyOpenGL-accelerate==3.1.*\n","\n","!pip install gym[box2d]==0.17.*\n","\n","import pyvirtualdisplay\n","\n","_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n","_ = _display.start()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BvN4S8R53mJI"},"source":["# Taken (partially) from \n","# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\n","\n","\n","model='PongNoFrameskip-v4-best.dat'\n","record_folder=\"video\"  \n","visualize=True\n","\n","env = make_env(DEFAULT_ENV_NAME)\n","if record_folder:\n","        env = gym.wrappers.Monitor(env, record_folder, force=True)\n","net = DQN(env.observation_space.shape, env.action_space.n)\n","net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n","\n","state = env.reset()\n","total_reward = 0.0\n","\n","while True:\n","        start_ts = time.time()\n","        if visualize:\n","            env.render()\n","        state_v = torch.tensor(np.array([state], copy=False))\n","        q_vals = net(state_v).data.numpy()[0]\n","        action = np.argmax(q_vals)\n","        \n","        state, reward, done, _ = env.step(action)\n","        total_reward += reward\n","        if done:\n","            break\n","        if visualize:\n","            delta = 1/FPS - (time.time() - start_ts)\n","            if delta > 0:\n","                time.sleep(delta)\n","print(\"Total reward: %.2f\" % total_reward)\n","\n","if record_folder:\n","        env.close()"],"execution_count":null,"outputs":[]}]}