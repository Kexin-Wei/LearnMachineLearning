{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari Breakout DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T14:01:04.020841Z",
     "iopub.status.busy": "2020-12-08T14:01:04.020560Z",
     "iopub.status.idle": "2020-12-08T14:01:04.823768Z",
     "shell.execute_reply": "2020-12-08T14:01:04.823465Z",
     "shell.execute_reply.started": "2020-12-08T14:01:04.020767Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T14:01:06.594243Z",
     "iopub.status.busy": "2020-12-08T14:01:06.594010Z",
     "iopub.status.idle": "2020-12-08T14:01:06.714990Z",
     "shell.execute_reply": "2020-12-08T14:01:06.714578Z",
     "shell.execute_reply.started": "2020-12-08T14:01:06.594217Z"
    }
   },
   "outputs": [],
   "source": [
    "env_name = 'Breakout-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T14:01:07.666995Z",
     "iopub.status.busy": "2020-12-08T14:01:07.666774Z",
     "iopub.status.idle": "2020-12-08T14:01:07.670638Z",
     "shell.execute_reply": "2020-12-08T14:01:07.670159Z",
     "shell.execute_reply.started": "2020-12-08T14:01:07.666968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space:  4\n",
      "Observation space:  (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "N_ACT = env.action_space.n\n",
    "N_OB  = env.observation_space.shape\n",
    "print(\"Action space: \",N_ACT)\n",
    "print(\"Observation space: \", N_OB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T14:05:19.577617Z",
     "iopub.status.busy": "2020-12-08T14:05:19.577399Z",
     "iopub.status.idle": "2020-12-08T14:05:19.596786Z",
     "shell.execute_reply": "2020-12-08T14:05:19.596308Z",
     "shell.execute_reply.started": "2020-12-08T14:05:19.577591Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    def __init__(self, N_ACT, N_OB, MEMORY_SIZE = 2000,BATCH_SIZE = 32, EPSILON = 0.1, GAMMA=0.9):\n",
    "        self.N_ACT   = N_ACT\n",
    "        self.N_OB    = N_OB\n",
    "        \n",
    "        self.EPSILON = EPSILON\n",
    "        self.GAMMA   = GAMMA\n",
    "        \n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.MEMORY_SIZE = MEMORY_SIZE\n",
    "        \n",
    "        self.model  = self.create_cnn()\n",
    "        \n",
    "        self.target_model = self.create_cnn()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen = MEMORY_SIZE)\n",
    "        \n",
    "    def create_cnn(self):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(8,3, padding = 'same', activation = 'relu', input_shape = self.N_OB),\n",
    "            tf.keras.layers.MaxPool2D(2, strides = 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(self.N_ACT, activation = 'linear')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            loss = 'huber_loss',\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "            metrics   = ['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    #==== get q-value\n",
    "    def get_q(self, ob):\n",
    "        return self.model.predict(ob.reshape(-1,self.N_OB[0],self.N_OB[1],self.N_OB[2])/255.0)\n",
    "    \n",
    "    #==== act = take_action()\n",
    "    def take_action(self,ob): \n",
    "        if np.random.rand() < self.EPSILON:\n",
    "            return np.random.randint(self.N_ACT)\n",
    "        q_value = self.get_q(ob)\n",
    "        return np.argmax(q_value)\n",
    "    \n",
    "    \n",
    "    #===== self.replay_memory <- add(ob,act,reward,ob_next)\n",
    "    def memorize(self,a_set_memory): \n",
    "        # a_set_memory = sars(a) : [ob, act, reward, ob_next, done]\n",
    "        self.replay_memory.append(a_set_memory)\n",
    "    \n",
    "    #==== batch train \n",
    "    def train(self):       \n",
    "        \n",
    "        batch_memory = random.sample(self.replay_memory,self.BATCH_SIZE)\n",
    "        \n",
    "        batch_ob  = np.array([ a_set_memory[0] for a_set_memory in batch_memory])/255\n",
    "        \n",
    "        batch_ob_next   = np.array([ a_set_memory[3] for a_set_memory in batch_memory])/255        \n",
    "        batch_q_next  = self.target_model.predict(batch_ob_next)\n",
    "        #set_trace()\n",
    "        batch_q_new = []\n",
    "        # loss = (reward+ q'-q)^2/batch_size\n",
    "        for index,(ob, act, reward, ob_next, done) in enumerate(batch_memory):\n",
    "            if not done:\n",
    "                q_next_max = np.max(batch_q_next[index])\n",
    "                q_new    = reward + self.GAMMA * q_next_max\n",
    "            else:\n",
    "                q_new    = reward \n",
    "            batch_q_new.append(q_new)\n",
    "             \n",
    "        self.model.fit(batch_ob,np.array(batch_q_new),batch_size = self.BATCH_SIZE, verbose = 0)\n",
    "        \n",
    "    \n",
    "    #==== target_model <- model\n",
    "    def target_model_update(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make dir path for log and figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T14:01:11.990360Z",
     "iopub.status.busy": "2020-12-08T14:01:11.990135Z",
     "iopub.status.idle": "2020-12-08T14:01:11.994341Z",
     "shell.execute_reply": "2020-12-08T14:01:11.993895Z",
     "shell.execute_reply.started": "2020-12-08T14:01:11.990334Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = \"../../gym_graph\"\n",
    "DIR = os.path.join(ROOT_DIR,datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(DIR)\n",
    "\n",
    "try:\n",
    "    os.makedirs('DQN_log')\n",
    "except:\n",
    "    pass\n",
    "log_file = open('DQN_log/log_'+datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "agent = DQN_Agent(N_ACT,N_OB)\n",
    "\n",
    "reward_summary = {\n",
    "    'max':[],\n",
    "    'min':[],\n",
    "    'ave':[]\n",
    "}\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    ob = env.reset()\n",
    "    \n",
    "    all_reward = []\n",
    "    step = 0\n",
    "    while(1):\n",
    "        # render monitoring\n",
    "        if ep % int(EPOCHS/10) == 0: #save 10 epoch move\n",
    "            os.makedirs(os.path.join(DIR,str(ep)))\n",
    "            plt.imsave(os.path.join(DIR,str(ep),str(step)+'.png'),env.render(mode='rgb_array'))\n",
    "        # take action\n",
    "        act = agent.take_action(ob)\n",
    "        \n",
    "        # env step\n",
    "        ob_next, reward, done, info = env.step(act)\n",
    "            # reward modified\n",
    "            # reward = reward if done else -1\n",
    "        \n",
    "        # memorize: sars(a) : [ob, act, reward, ob_next, done]\n",
    "        agent.memorize([ob, act, reward, ob_next, done])\n",
    "        \n",
    "        # q-value update\n",
    "        if len(agent.replay_memory) > (agent.MEMORY_SIZE/10):\n",
    "            agent.train()            \n",
    "            if step % 5 == 0:\n",
    "                #set_trace()\n",
    "                agent.target_model_update()\n",
    "            \n",
    "        ob = ob_next\n",
    "        all_reward.append(reward)\n",
    "        step += 1\n",
    "        \n",
    "        if done:\n",
    "            #set_trace()\n",
    "            log_file.write(\"Epoch {} - average rewards {} with step {}\\n\".format(ep,sum(all_reward)/len(all_reward),step))\n",
    "            print(\"Epoch {} - average rewards {} with step {}\\n\".format(ep,sum(all_reward)/len(all_reward),step))\n",
    "            reward_summary['max'].append(max(all_reward))\n",
    "            reward_summary['min'].append(min(all_reward))\n",
    "            reward_summary['ave'].append(sum(all_reward)/len(all_reward))\n",
    "            break\n",
    "            \n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the final run\n",
    "ob = env.reset()\n",
    "all_reward = 0\n",
    "step = 0\n",
    "while(1):\n",
    "    os.makedirs(os.path.join(DIR,'final'))\n",
    "    plt.imsave(os.path.join(DIR,'final',str(step)+'.png'),env.render(mode='rgb_array'))\n",
    "    act = np.argmax(agent.model.predict(ob))\n",
    "    \n",
    "    ob,reward,done,infor = env.step(act)\n",
    "    \n",
    "    all_reward +=reward\n",
    "    step +=1\n",
    "    if done:\n",
    "        print('Final: rewards - {}, step - {}'.format(all_reward,step))\n",
    "        break\n",
    "        \n",
    "\n",
    "env.close()\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
